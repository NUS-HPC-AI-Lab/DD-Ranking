<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>DD-Ranking Benchmark</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">DD-Ranking Benchmark</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p>DD-Ranking (DD, <em>i.e.</em>, Dataset Distillation) is an integrated and easy-to-use evaluation benchmark for dataset distillation. It aims to provide a fair evaluation scheme for DD methods that can decouple the impacts from knowledge distillation and data augmentation to reflect the real informativeness of the distilled data.</p>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>Dataset Distillation (DD) aims to condense a large dataset into a much smaller one, which allows a model to achieve comparable performance after training on it. DD has gained extensive attention since it was proposed. With some foundational methods such as DC, DM, and MTT, various works have further pushed this area to a new standard with their novel designs.</p>
<p><img src="static/history.png" alt="history" /></p>
<p>Notebaly, more and more methods are transitting from "hard label" to "soft label" in dataset distillation, especially during evaluation. <strong>Hard labels</strong> are categorical, having the same format of the real dataset. <strong>Soft labels</strong> are distributions, typically generated by a pre-trained teacher model.
Recently, Deng et al., pointed out that "a label is worth a thousand images". They showed analytically that soft labels are exetremely useful for accuracy improvement.</p>
<p>However, since the essence of soft labels is <strong>knowledge distillation</strong>, we want to ask a question: <strong>Can the test accuracy of the model trained on distilled data reflect the real informativeness of the distilled data?</strong></p>
<p>Specifically, we have discoverd unfairness of using only test accuracy to demonstrate one's performance from the following three aspects:</p>
<ol>
<li>Results of using hard and soft labels are not directly comparable since soft labels introduce teacher knowledge.</li>
<li>Strategies of using soft labels are diverse. For instance, different objective functions are used during evaluation, such as soft Cross-Entropy and Kullbackâ€“Leibler divergence. Also, one image may be mapped to one or multiple soft labels.</li>
<li>Different data augmentations are used during evaluation.</li>
</ol>
<p>Motivated by this, we propose DD-Ranking, a new benchmark for DD evaluation. DD-Ranking provides a fair evaluation scheme for DD methods that can decouple the impacts from knowledge distillation and data augmentation to reflect the real informativeness of the distilled data.</p>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li><strong>Fair Evaluation</strong>: DD-Ranking provides a fair evaluation scheme for DD methods that can decouple the impacts from knowledge distillation and data augmentation to reflect the real informativeness of the distilled data.</li>
<li><strong>Easy-to-use</strong>: DD-Ranking provides a unified interface for dataset distillation evaluation.</li>
<li><strong>Extensible</strong>: DD-Ranking supports various datasets and models.</li>
<li><strong>Customizable</strong>: DD-Ranking supports various data augmentations and soft label strategies.</li>
</ul>
<h2 id="dd-ranking-score"><a class="header" href="#dd-ranking-score">DD-Ranking Score</a></h2>
<p>Revisit the original goal of dataset distillation:</p>
<blockquote>
<p>The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data.</p>
</blockquote>
<p>The evaluation method for DD-Ranking is grounded in the essence of dataset distillation, aiming to better reflect the information content of the synthesized data by assessing the following two aspects:</p>
<ol>
<li>
<p>The degree to which the original dataset is recovered under hard labels (hard label recovery): \( \text{HLR} = \text{Acc.} \text{full-hard} - \text{Acc.} \text{syn-hard} \)</p>
</li>
<li>
<p>The improvement over random selection when using personalized evaluation methods (improvement over random): \( \text{IOR} = \text{Acc.} \text{syn-any} - \text{Acc.} \text{rdm-any} \)</p>
</li>
</ol>
<p>\(\text{Acc.}\) is the accuracy of models trained on different samples. Samples' marks are as follows:</p>
<ul>
<li>\(\text{full-hard}\): Full dataset with hard labels;</li>
<li>\(\text{syn-hard}\): Synthetic dataset with hard labels;</li>
<li>\(\text{syn-any}\): Synthetic dataset with personalized evaluation methods (hard or soft labels);</li>
<li>\(\text{rdm-any}\): Randomly selected dataset (under the same compression ratio) with the same personalized evaluation methods.</li>
</ul>
<p>To rank different methods, we combine the above two metrics as DD-Ranking Score:</p>
<p>\[\text{DD-Ranking Score} = \frac{\text{IOR}}{\text{HLR}} = \frac{(\text{Acc.} \text{syn-any}-\text{Acc.} \text{rdm-any})}{(\text{Acc.} \text{full-hard}-\text{Acc.} \text{syn-hard})}\]</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>From pip</p>
<pre><code class="language-bash">pip install dd_ranking
</code></pre>
<p>From source</p>
<pre><code class="language-bash">python setup.py install
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<p>Below is a step-by-step guide on how to use our <code>dd_ranking</code>. This demo is based on soft labels (source code can be found in <code>demo_soft.py</code>). You can find hard label demo in <code>demo_hard.py</code>.</p>
<p><strong>Step1</strong>: Intialize a soft-label metric evaluator object. Config files are recommended for users to specify hyper-parameters. Sample config files are provided <a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/tree/main/configs">here</a>.</p>
<pre><code class="language-python">from dd_ranking.metrics import Soft_Label_Evaluator
from dd_ranking.config import Config

&gt;&gt;&gt; config = Config.from_file("./configs/Demo_Soft_Label.yaml")
&gt;&gt;&gt; soft_obj = Soft_Label_Evaluator(config)
</code></pre>
<details>
<summary>You can also pass keyword arguments.</summary>
<pre><code class="language-python">device = "cuda"
method_name = "DATM"                    # Specify your method name
ipc = 10                                # Specify your IPC
dataset = "CIFAR10"                     # Specify your dataset name
syn_data_dir = "./data/CIFAR10/IPC10/"  # Specify your synthetic data path
real_data_dir = "./datasets"            # Specify your dataset path
model_name = "ConvNet-3"                # Specify your model name
teacher_dir = "./teacher_models"		# Specify your path to teacher model chcekpoints
im_size = (32, 32)                      # Specify your image size
dsa_params = {                          # Specify your data augmentation parameters
    "prob_flip": 0.5,
    "ratio_rotate": 15.0,
    "saturation": 2.0,
    "brightness": 1.0,
    "contrast": 0.5,
    "ratio_scale": 1.2,
    "ratio_crop_pad": 0.125,
    "ratio_cutout": 0.5
}
save_path = f"./results/{dataset}/{model_name}/IPC{ipc}/dm_hard_scores.csv"

""" We only list arguments that usually need specifying"""
soft_label_metric_calc = Soft_Label_Evaluator(
    dataset=dataset,
    real_data_path=real_data_dir, 
    ipc=ipc,
    model_name=model_name,
    soft_label_criterion='sce',  # Use Soft Cross Entropy Loss
    soft_label_mode='S',         # Use one-to-one image to soft label mapping
    data_aug_func='dsa',         # Use DSA data augmentation
    aug_params=dsa_params,       # Specify dsa parameters
    im_size=im_size,
    stu_use_torchvision=False,
    tea_use_torchvision=False,
    teacher_dir='./teacher_models',
    device=device,
    save_path=save_path
)
</code></pre>
</details>
<p>For detailed explanation for hyper-parameters, please refer to our <a href="">documentation</a>.</p>
<p><strong>Step 2:</strong> Load your synthetic data, labels (if any), and learning rate (if any).</p>
<pre><code class="language-python">&gt;&gt;&gt; syn_images = torch.load('/your/path/to/syn/images.pt')
# You must specify your soft labels if your soft label mode is 'S'
&gt;&gt;&gt; soft_labels = torch.load('/your/path/to/syn/labels.pt')
&gt;&gt;&gt; syn_lr = torch.load('/your/path/to/syn/lr.pt')
</code></pre>
<p><strong>Step 3:</strong> Compute the xxx metric.</p>
<pre><code class="language-python">&gt;&gt;&gt; metric = soft_label_metric_calc.compute_metrics(image_tensor=syn_images, soft_labels=soft_labels, syn_lr=syn_lr)
</code></pre>
<p>The following results will be returned to you:</p>
<ul>
<li><code>hard_label_recovery mean</code>: The mean of hard label recovery scores.</li>
<li><code>hard_label_recovery std</code>: The standard deviation of hard label recovery scores.</li>
<li><code>improvement_over_random mean</code>: The mean of improvement over random scores.</li>
<li><code>improvement_over_random std</code>: The standard deviation of improvement over random scores.</li>
<li><code>dd_ranking_score mean</code>: The mean of dd ranking scores.</li>
<li><code>dd_ranking_score std</code>: The standard deviation of dd ranking scores.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dd-ranking-metrics"><a class="header" href="#dd-ranking-metrics">DD-Ranking Metrics</a></h1>
<p>DD-Ranking provides a set of metrics to evaluate the real informativeness of datasets distilled by different methods. We categorize dataset distillation methods by the type of labels they use: hard label and soft label. For each label type, we provide a evaluation class that computes hard label recovery (HLR), improvement over random (IOR), and the DD-Ranking score. Additionally, we provide a general evaluation class, integrating most of exisiting evaluation methods, that computes the traditional test accuracy.</p>
<ul>
<li><a href="metrics/hard-label.html">Hard_Label_Evaluator</a> computes HLR, IOR, and DD-Ranking score for methods using hard labels.</li>
<li><a href="metrics/soft-label.html">Soft_Label_Evaluator</a> computes HLR, IOR, and DD-Ranking score for methods using soft labels.</li>
<li><a href="metrics/general.html">General_Evaluator</a> computes the traditional test accuracy for existing methods.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="hard-label-evaluator"><a class="header" href="#hard-label-evaluator">Hard Label Evaluator</a></h2>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p><span style="color:#FF6B00;">CLASS</span>
dd_ranking.metrics.Hard_Label_Evaluator(config=None,
dataset: str = 'CIFAR10',
real_data_path: str = './dataset/',
ipc: int = 10,
model_name: str = 'ConvNet-3',
data_aug_func: str = 'cutmix',
aug_params: dict = {'cutmix_p': 1.0},
optimizer: str = 'sgd',
lr_scheduler: str = 'step',
weight_decay: float = 0.0005,
momentum: float = 0.9,
use_zca: bool = False,
num_eval: int = 5,
im_size: tuple = (32, 32),
num_epochs: int = 300,
real_batch_size: int = 256,
syn_batch_size: int = 256,
use_torchvision: bool = False,
default_lr: float = 0.01,
num_workers: int = 4,
save_path: Optional[str] = None,
custom_train_trans = None,
custom_val_trans = None,
device: str = "cuda"
)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/metrics/hard_label.py"><strong>[SOURCE]</strong></a></p>
</div>
<p>A class for evaluating the performance of a dataset distillation method with hard labels. User is able to modify the attributes as needed.</p>
<h3 id="parameters"><a class="header" href="#parameters">Parameters</a></h3>
<ul>
<li><strong>config</strong>(<span style="color:#FF6B00;">Optional[Config]</span>): Config object for specifying all attributes. See <a href="metrics/../config/overview.html">config</a> for more details.</li>
<li><strong>dataset</strong>(<span style="color:#FF6B00;">str</span>): Name of the real dataset.</li>
<li><strong>real_data_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the real dataset.</li>
<li><strong>ipc</strong>(<span style="color:#FF6B00;">int</span>): Images per class.</li>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the surrogate model. See <a href="metrics/../models/overview.html">models</a> for more details.</li>
<li><strong>data_aug_func</strong>(<span style="color:#FF6B00;">str</span>): Data augmentation function used during training. Currently supports <code>dsa</code>, <code>cutmix</code>, <code>mixup</code>. See <a href="metrics/../augmentations/overview.html">augmentations</a> for more details.</li>
<li><strong>aug_params</strong>(<span style="color:#FF6B00;">dict</span>): Parameters for the data augmentation function.</li>
<li><strong>optimizer</strong>(<span style="color:#FF6B00;">str</span>): Name of the optimizer. Currently supports torch-based optimizers - <code>sgd</code>, <code>adam</code>, and <code>adamw</code>.</li>
<li><strong>lr_scheduler</strong>(<span style="color:#FF6B00;">str</span>): Name of the learning rate scheduler. Currently supports torch-based schedulers - <code>step</code>, <code>cosine</code>, <code>lambda_step</code>, and <code>lambda_cos</code>.</li>
<li><strong>weight_decay</strong>(<span style="color:#FF6B00;">float</span>): Weight decay for the optimizer.</li>
<li><strong>momentum</strong>(<span style="color:#FF6B00;">float</span>): Momentum for the optimizer.</li>
<li><strong>use_zca</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use ZCA whitening.</li>
<li><strong>num_eval</strong>(<span style="color:#FF6B00;">int</span>): Number of evaluations to perform.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Size of the images.</li>
<li><strong>num_epochs</strong>(<span style="color:#FF6B00;">int</span>): Number of epochs to train.</li>
<li><strong>real_batch_size</strong>(<span style="color:#FF6B00;">int</span>): Batch size for the real dataset.</li>
<li><strong>syn_batch_size</strong>(<span style="color:#FF6B00;">int</span>): Batch size for the synthetic dataset.</li>
<li><strong>use_torchvision</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use torchvision to initialize the model.</li>
<li><strong>default_lr</strong>(<span style="color:#FF6B00;">float</span>): Default learning rate for the optimizer, typically used for training on the real dataset.</li>
<li><strong>num_workers</strong>(<span style="color:#FF6B00;">int</span>): Number of workers for data loading.</li>
<li><strong>save_path</strong>(<span style="color:#FF6B00;">Optional[str]</span>): Path to save the results.</li>
<li><strong>custom_train_trans</strong>(<span style="color:#FF6B00;">Optional[Callable]</span>): Custom transformation function when loading synthetic data. Only support torchvision transformations.</li>
<li><strong>custom_val_trans</strong>(<span style="color:#FF6B00;">Optional[Callable]</span>): Custom transformation function when loading test dataset. Only support torchvision transformations.</li>
<li><strong>device</strong>(<span style="color:#FF6B00;">str</span>): Device to use for evaluation, <code>cuda</code> or <code>cpu</code>.</li>
</ul>
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px; margin-left:15px; margin-right:15px;">
<p>compute_metrics(image_tensor: Tensor = None, image_path: str = None, hard_labels: Tensor = None, syn_lr: float = None)</p>
</div>
<p>This method computes the HLR, IOR, and DD-Ranking scores for the given image and soft labels (if provided). In each evaluation round, we set a different random seed and perform the following steps:</p>
<ol>
<li>Compute the test accuracy of the surrogate model on the synthetic dataset under hard labels. We tune the learning rate for the best performance if <code>syn_lr</code> is not provided.</li>
<li>Compute the test accuracy of the surrogate model on the real dataset under the same setting as step 1.</li>
<li>Compute the test accuracy of the surrogate model on the randomly selected dataset under the same setting as step 1.</li>
<li>Compute the HLR, IOR, and DD-Ranking scores.</li>
</ol>
<p>The final scores are the average of the scores from <code>num_eval</code> rounds.</p>
<h4 id="parameters-1"><a class="header" href="#parameters-1">Parameters</a></h4>
<ul>
<li><strong>image_tensor</strong>(<span style="color:#FF6B00;">Tensor</span>): Image tensor. Must specify when <code>image_path</code> is not provided. We require the shape to be <code>(N x IPC, C, H, W)</code> where <code>N</code> is the number of classes.</li>
<li><strong>image_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the image. Must specify when <code>image_tensor</code> is not provided.</li>
<li><strong>hard_labels</strong>(<span style="color:#FF6B00;">Tensor</span>): Hard label tensor. The first dimension must be the same as <code>image_tensor</code>.</li>
<li><strong>syn_lr</strong>(<span style="color:#FF6B00;">float</span>): Learning rate for the synthetic dataset. If not specified, the learning rate will be tuned automatically.</li>
</ul>
<h4 id="returns"><a class="header" href="#returns">Returns</a></h4>
<p>A dictionary with the following keys:</p>
<ul>
<li><strong>hard_label_recovery_mean</strong>: Mean of HLR scores from <code>num_eval</code> rounds.</li>
<li><strong>hard_label_recovery_std</strong>: Standard deviation of HLR scores from <code>num_eval</code> rounds.</li>
<li><strong>improvement_over_random_mean</strong>: Mean of improvement over random scores from <code>num_eval</code> rounds.</li>
<li><strong>improvement_over_random_std</strong>: Standard deviation of improvement over random scores from <code>num_eval</code> rounds.</li>
<li><strong>dd_ranking_mean</strong>: Mean of DD-Ranking scores from <code>num_eval</code> rounds.</li>
<li><strong>dd_ranking_std</strong>: Standard deviation of DD-Ranking scores from <code>num_eval</code> rounds.</li>
</ul>
<p><strong>Examples:</strong></p>
<p>with config file:</p>
<pre><code class="language-python">&gt;&gt;&gt; config = Config('/path/to/config.yaml')
&gt;&gt;&gt; evaluator = Hard_Label_Evaluator(config=config)
# load the image and hard labels
&gt;&gt;&gt; image_tensor, hard_labels = ...
# compute the metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, hard_labels=hard_labels)
# alternatively, you can provide the image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.png', hard_labels=hard_labels)
</code></pre>
<p>with keyword arguments:</p>
<pre><code class="language-python">&gt;&gt;&gt; evaluator = Hard_Label_Evaluator(
...     dataset='CIFAR10',
...     model_name='ConvNet-3',
...     data_aug_func='dsa',
...     aug_params={
...         "prob_flip": 0.5,
...         "ratio_rotate": 15.0,
...         "saturation": 2.0,
...         "brightness": 1.0,
...         "contrast": 0.5,
...         "ratio_scale": 1.2,
...         "ratio_crop_pad": 0.125,
...         "ratio_cutout": 0.5
...     },
...     optimizer='sgd',
...     lr_scheduler='step',
...     weight_decay=0.0005,
...     momentum=0.9,
...     use_zca=False,
...     num_eval=5,
...     device='cuda'
... )
# load the image and hard labels
&gt;&gt;&gt; image_tensor, hard_labels = ...
# compute the metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, hard_labels=hard_labels)
# alternatively, you can provide the image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.png', hard_labels=hard_labels)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="soft-label-evaluator"><a class="header" href="#soft-label-evaluator">Soft Label Evaluator</a></h2>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p><span style="color:#FF6B00;">CLASS</span>
dd_ranking.metrics.Soft_Label_Evaluator(config: Optional[Config] = None,
dataset: str = 'CIFAR10',
real_data_path: str = './dataset/',
ipc: int = 10,
model_name: str = 'ConvNet-3',
soft_label_mode: str='S',
soft_label_criterion: str='kl',
temperature: float=1.0,
data_aug_func: str='cutmix',
aug_params: dict={'cutmix_p': 1.0},
optimizer: str='sgd',
lr_scheduler: str='step',
weight_decay: float=0.0005,
momentum: float=0.9,
num_eval: int=5,
im_size: tuple=(32, 32),
num_epochs: int=300,
use_zca: bool=False,
real_batch_size: int=256,
syn_batch_size: int=256,
default_lr: float=0.01,
save_path: str=None,
stu_use_torchvision: bool=False,
tea_use_torchvision: bool=False,
num_workers: int=4,
teacher_dir: str='./teacher_models',
custom_train_trans: Optional[Callable]=None,
custom_val_trans: Optional[Callable]=None,
device: str="cuda"
)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/metrics/soft_label.py"><strong>[SOURCE]</strong></a></p>
</div>
<p>A class for evaluating the performance of a dataset distillation method with soft labels. User is able to modify the attributes as needed.</p>
<h3 id="parameters-2"><a class="header" href="#parameters-2">Parameters</a></h3>
<ul>
<li><strong>config</strong>(<span style="color:#FF6B00;">Optional[Config]</span>): Config object for specifying all attributes. See <a href="metrics/../config/overview.html">config</a> for more details.</li>
<li><strong>dataset</strong>(<span style="color:#FF6B00;">str</span>): Name of the real dataset.</li>
<li><strong>real_data_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the real dataset.</li>
<li><strong>ipc</strong>(<span style="color:#FF6B00;">int</span>): Images per class.</li>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the surrogate model. See <a href="metrics/../models/overview.html">models</a> for more details.</li>
<li><strong>soft_label_mode</strong>(<span style="color:#FF6B00;">str</span>): Number of soft labels per image. <code>S</code> for single soft label, <code>M</code> for multiple soft labels.</li>
<li><strong>soft_label_criterion</strong>(<span style="color:#FF6B00;">str</span>): Loss function for using soft labels. Currently supports <code>kl</code> for KL divergence, <code>sce</code> for soft cross-entropy.</li>
<li><strong>temperature</strong>(<span style="color:#FF6B00;">float</span>): Temperature for knowledge distillation.</li>
<li><strong>data_aug_func</strong>(<span style="color:#FF6B00;">str</span>): Data augmentation function used during training. Currently supports <code>dsa</code>, <code>cutmix</code>, <code>mixup</code>. See <a href="metrics/../augmentations/overview.html">augmentations</a> for more details.</li>
<li><strong>aug_params</strong>(<span style="color:#FF6B00;">dict</span>): Parameters for the data augmentation function.</li>
<li><strong>optimizer</strong>(<span style="color:#FF6B00;">str</span>): Name of the optimizer. Currently supports torch-based optimizers - <code>sgd</code>, <code>adam</code>, and <code>adamw</code>.</li>
<li><strong>lr_scheduler</strong>(<span style="color:#FF6B00;">str</span>): Name of the learning rate scheduler. Currently supports torch-based schedulers - <code>step</code>, <code>cosine</code>, <code>lambda_step</code>, and <code>lambda_cos</code>.</li>
<li><strong>weight_decay</strong>(<span style="color:#FF6B00;">float</span>): Weight decay for the optimizer.</li>
<li><strong>momentum</strong>(<span style="color:#FF6B00;">float</span>): Momentum for the optimizer.</li>
<li><strong>use_zca</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use ZCA whitening.</li>
<li><strong>num_eval</strong>(<span style="color:#FF6B00;">int</span>): Number of evaluations to perform.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Size of the images.</li>
<li><strong>num_epochs</strong>(<span style="color:#FF6B00;">int</span>): Number of epochs to train.</li>
<li><strong>real_batch_size</strong>(<span style="color:#FF6B00;">int</span>): Batch size for the real dataset.</li>
<li><strong>syn_batch_size</strong>(<span style="color:#FF6B00;">int</span>): Batch size for the synthetic dataset.</li>
<li><strong>stu_use_torchvision</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use torchvision to initialize the student model.</li>
<li><strong>tea_use_torchvision</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use torchvision to initialize the teacher model.</li>
<li><strong>teacher_dir</strong>(<span style="color:#FF6B00;">str</span>): Path to the teacher model.</li>
<li><strong>default_lr</strong>(<span style="color:#FF6B00;">float</span>): Default learning rate for the optimizer, typically used for training on the real dataset.</li>
<li><strong>num_workers</strong>(<span style="color:#FF6B00;">int</span>): Number of workers for data loading.</li>
<li><strong>save_path</strong>(<span style="color:#FF6B00;">Optional[str]</span>): Path to save the results.</li>
<li><strong>custom_train_trans</strong>(<span style="color:#FF6B00;">Optional[Callable]</span>): Custom transformation function when loading synthetic data. Only support torchvision transformations.</li>
<li><strong>custom_val_trans</strong>(<span style="color:#FF6B00;">Optional[Callable]</span>): Custom transformation function when loading test dataset. Only support torchvision transformations.</li>
<li><strong>device</strong>(<span style="color:#FF6B00;">str</span>): Device to use for evaluation, <code>cuda</code> or <code>cpu</code>.</li>
</ul>
<h3 id="methods-1"><a class="header" href="#methods-1">Methods</a></h3>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px; margin-left:15px; margin-right:15px;">
<p>compute_metrics(image_tensor: Tensor = None, image_path: str = None, soft_labels: Tensor = None, syn_lr: float = None)</p>
</div>
<div style="margin-left:15px; margin-right:15px;">
This method computes the HLR, IOR, and DD-Ranking scores for the given image and soft labels (if provided). In each evaluation round, we set a different random seed and perform the following steps:
<ol>
<li>Compute the test accuracy of the surrogate model on the synthetic dataset under hard labels. We perform learning rate tuning for the best performance.</li>
<li>Compute the test accuracy of the surrogate model on the real dataset under the same setting as step 1.</li>
<li>Compute the test accuracy of the surrogate model on the synthetic dataset under soft labels.</li>
<li>Compute the test accuracy of the surrogate model on the randomly selected dataset under the same setting as step 3.</li>
<li>Compute the HLR, IOR, and DD-Ranking scores.</li>
</ol>
<p>The final scores are the average of the scores from <code>num_eval</code> rounds.</p>
<h4 id="parameters-3"><a class="header" href="#parameters-3">Parameters</a></h4>
<ul>
<li><strong>image_tensor</strong>(<span style="color:#FF6B00;">Tensor</span>): Image tensor. Must specify when <code>image_path</code> is not provided. We require the shape to be <code>(N x IPC, C, H, W)</code> where <code>N</code> is the number of classes.</li>
<li><strong>image_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the image. Must specify when <code>image_tensor</code> is not provided.</li>
<li><strong>soft_labels</strong>(<span style="color:#FF6B00;">Tensor</span>): Soft label tensor. Must specify when <code>soft_label_mode</code> is <code>S</code>. The first dimension must be the same as <code>image_tensor</code>.</li>
<li><strong>syn_lr</strong>(<span style="color:#FF6B00;">float</span>): Learning rate for the synthetic dataset. If not specified, the learning rate will be tuned automatically.</li>
</ul>
<h4 id="returns-1"><a class="header" href="#returns-1">Returns</a></h4>
<p>A dictionary with the following keys:</p>
<ul>
<li><strong>hard_label_recovery_mean</strong>: Mean of HLR scores from <code>num_eval</code> rounds.</li>
<li><strong>hard_label_recovery_std</strong>: Standard deviation of HLR scores from <code>num_eval</code> rounds.</li>
<li><strong>improvement_over_random_mean</strong>: Mean of improvement over random scores from <code>num_eval</code> rounds.</li>
<li><strong>improvement_over_random_std</strong>: Standard deviation of improvement over random scores from <code>num_eval</code> rounds.</li>
<li><strong>dd_ranking_mean</strong>: Mean of DD-Ranking scores from <code>num_eval</code> rounds.</li>
<li><strong>dd_ranking_std</strong>: Standard deviation of DD-Ranking scores from <code>num_eval</code> rounds.</li>
</ul>
</div>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<p>with config file:</p>
<pre><code class="language-python">&gt;&gt;&gt; config = Config('/path/to/config.yaml')
&gt;&gt;&gt; evaluator = Soft_Label_Evaluator(config=config)
# load image and soft labels
&gt;&gt;&gt; image_tensor, soft_labels = ... 
# compute metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, soft_labels=soft_labels)
# alternatively, provide image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.jpg', soft_labels=soft_labels) 
</code></pre>
<p>with keyword arguments:</p>
<pre><code class="language-python">&gt;&gt;&gt; evaluator = Soft_Label_Evaluator(
...     dataset='TinyImageNet',
...     model_name='ResNet-18-BN',
...     soft_label_mode='M',
...     soft_label_criterion='kl',
...     temperature=10.0,
...     data_aug_func='mixup',
...     aug_params={
...         "mixup_p": 0.8,
...     },
...     optimizer='sgd',
...     lr_scheduler='step',
...     weight_decay=0.0005,
...     momentum=0.9,
...     stu_use_torchvision=True,
...     tea_use_torchvision=True,
...     num_eval=5,
...     device='cuda'
... )
# load image and soft labels
&gt;&gt;&gt; image_tensor, soft_labels = ... 
# compute metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, soft_labels=soft_labels)
# alternatively, provide image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.jpg', soft_labels=soft_labels) 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="general-evaluator"><a class="header" href="#general-evaluator">General Evaluator</a></h2>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p><span style="color:#FF6B00;">CLASS</span>
dd_ranking.metrics.General_Evaluator(config: Optional[Config] = None,
dataset: str = 'CIFAR10',
real_data_path: str = './dataset/',
ipc: int = 10,
model_name: str = 'ConvNet-3',
soft_label_mode: str='S',
soft_label_criterion: str='kl',
temperature: float=1.0,
data_aug_func: str='cutmix',
aug_params: dict={'cutmix_p': 1.0},
optimizer: str='sgd',
lr_scheduler: str='step',
weight_decay: float=0.0005,
momentum: float=0.9,
num_eval: int=5,
im_size: tuple=(32, 32),
num_epochs: int=300,
use_zca: bool=False,
real_batch_size: int=256,
syn_batch_size: int=256,
default_lr: float=0.01,
save_path: str=None,
stu_use_torchvision: bool=False,
tea_use_torchvision: bool=False,
num_workers: int=4,
teacher_dir: str='./teacher_models',
custom_train_trans: Optional[Callable]=None,
custom_val_trans: Optional[Callable]=None,
device: str="cuda"
)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/metrics/general.py"><strong>[SOURCE]</strong></a></p>
</div>
<p>A class for evaluating the traditional test accuracy of a surrogate model on the synthetic dataset under various settings (label type, data augmentation, etc.).</p>
<h3 id="parameters-4"><a class="header" href="#parameters-4">Parameters</a></h3>
<p>Same as <a href="metrics/soft-label.html">Soft Label Evaluator</a>.</p>
<h3 id="methods-2"><a class="header" href="#methods-2">Methods</a></h3>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px; margin-left:15px; margin-right:15px;">
<p>compute_metrics(image_tensor: Tensor = None, image_path: str = None, labels: Tensor = None, syn_lr: float = None)</p>
</div>
<div style="margin-left:15px; margin-right:15px;">
This method computes the test accuracy of the surrogate model on the synthetic dataset under various settings (label type, data augmentation, etc.).
<h4 id="parameters-5"><a class="header" href="#parameters-5">Parameters</a></h4>
<ul>
<li><strong>image_tensor</strong>(<span style="color:#FF6B00;">Tensor</span>): Image tensor. Must specify when <code>image_path</code> is not provided. We require the shape to be <code>(N x IPC, C, H, W)</code> where <code>N</code> is the number of classes.</li>
<li><strong>image_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the image. Must specify when <code>image_tensor</code> is not provided.</li>
<li><strong>labels</strong>(<span style="color:#FF6B00;">Tensor</span>): Label tensor. It can be either hard labels or soft labels. When <code>soft_label_mode=S</code>, the label tensor must be provided.</li>
<li><strong>syn_lr</strong>(<span style="color:#FF6B00;">float</span>): Learning rate for the synthetic dataset. If not specified, the learning rate will be tuned automatically.</li>
</ul>
<h4 id="returns-2"><a class="header" href="#returns-2">Returns</a></h4>
<p>A dictionary with the following keys:</p>
<ul>
<li><strong>acc_mean</strong>: Mean of test accuracy from <code>num_eval</code> rounds.</li>
<li><strong>acc_std</strong>: Standard deviation of test accuracy from <code>num_eval</code> rounds.</li>
</ul>
</div>
<h3 id="examples-1"><a class="header" href="#examples-1">Examples</a></h3>
<p>with config file:</p>
<pre><code class="language-python">&gt;&gt;&gt; config = Config('/path/to/config.yaml')
&gt;&gt;&gt; evaluator = General_Evaluator(config=config)
# load image and labels
&gt;&gt;&gt; image_tensor, labels = ... 
# compute metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, labels=labels)
# alternatively, provide image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.jpg', labels=labels) 
</code></pre>
<p>with keyword arguments:</p>
<pre><code class="language-python">&gt;&gt;&gt; evaluator = General_Evaluator(
...     dataset='CIFAR10',
...     model_name='ConvNet-3',
...     soft_label_mode='S',
...     soft_label_criterion='sce',
...     temperature=1.0,
...     data_aug_func='cutmix',
...     aug_params={
...         "cutmix_p": 1.0,
...     },
...     optimizer='sgd',
...     lr_scheduler='step',
...     weight_decay=0.0005,
...     momentum=0.9,
...     stu_use_torchvision=False,
...     tea_use_torchvision=False,
...     num_eval=5,
...     device='cuda'
... )
# load image and labels
&gt;&gt;&gt; image_tensor, labels = ... 
# compute metrics
&gt;&gt;&gt; evaluator.compute_metrics(image_tensor=image_tensor, labels=labels)
# alternatively, provide image path
&gt;&gt;&gt; evaluator.compute_metrics(image_path='path/to/image.jpg', labels=labels) 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="models"><a class="header" href="#models">Models</a></h1>
<p>DD-Ranking provides a set of commonly used model architectures in existing dataset distillation methods. Users can flexibly use these models for main evaluation or cross-architecture evaluation. We will keep updating this section with more models.</p>
<ul>
<li><a href="models/convnet.html">ConvNet</a></li>
<li><a href="models/resnet.html">ResNet</a></li>
<li><a href="models/vgg.html">VGG</a></li>
<li><a href="models/lenet.html">LeNet</a></li>
<li><a href="models/alexnet.html">AlexNet</a></li>
<li><a href="models/mlp.html">MLP</a></li>
</ul>
<h2 id="naming-convention"><a class="header" href="#naming-convention">Naming Convention</a></h2>
<p>We use the following naming convention for models in DD-Ranking:</p>
<ul>
<li><code>model name - model depth - norm type</code></li>
</ul>
<p>Model name and depth are required. When norm type is not specified, we use default normalization for the model. For example, <code>ResNet-18-BN</code> means ResNet18 with batch normalization. <code>ConvNet-4</code> means ConvNet with depth 4 and default instance normalization.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="convnet"><a class="header" href="#convnet">ConvNet</a></h2>
<p>Our <a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/networks.py">implementation</a> of ConvNet is based on <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a>.</p>
<p>By default, we use width 128, average pooling, and ReLU activation. We provide the following interface to initialize a ConvNet model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_convnet(model_name: str,
im_size: tuple, channel: int, num_classes: int, net_depth: int, net_norm: str, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-6"><a class="header" href="#parameters-6">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>net_depth</strong>(<span style="color:#FF6B00;">int</span>): Depth of the network.</li>
<li><strong>net_norm</strong>(<span style="color:#FF6B00;">str</span>): Normalization method. In ConvNet, we support <code>instance</code>, <code>batch</code>, and <code>group</code> normalization.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<p>To load a ConvNet model with different width or activation function or pooling method, you can use the following interface:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.networks.ConvNet(channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/networks.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-7"><a class="header" href="#parameters-7">Parameters</a></h3>
<p>We only list the parameters that are not present in <code>get_convnet</code>.</p>
<ul>
<li><strong>net_width</strong>(<span style="color:#FF6B00;">int</span>): Width of the network.</li>
<li><strong>net_act</strong>(<span style="color:#FF6B00;">str</span>): Activation function. We support <code>relu</code>, <code>leakyrelu</code>, and <code>sigmoid</code>.</li>
<li><strong>net_pooling</strong>(<span style="color:#FF6B00;">str</span>): Pooling method. We support <code>avgpooling</code>, <code>maxpooling</code>, and <code>none</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="alexnet"><a class="header" href="#alexnet">AlexNet</a></h2>
<p>Our <a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/networks.py">implementation</a> of ConvNet is based on <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a>.</p>
<p>We provide the following interface to initialize a AlexNet model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_alexnet(model_name: str, im_size: tuple, channel: int, num_classes: int, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-8"><a class="header" href="#parameters-8">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="resnet"><a class="header" href="#resnet">ResNet</a></h2>
<p>DD-Ranking supports implementation of ResNet in both <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a> and <a href="https://pytorch.org/vision/main/models/resnet.html">torchvision</a>.</p>
<p>We provide the following interface to initialize a ConvNet model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_resnet(model_name: str,
im_size: tuple, channel: int, num_classes: int, depth: int, batchnorm: bool, use_torchvision: bool, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-9"><a class="header" href="#parameters-9">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>depth</strong>(<span style="color:#FF6B00;">int</span>): Depth of the network.</li>
<li><strong>batchnorm</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use batch normalization.</li>
<li><strong>use_torchvision</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use torchvision to initialize the model. When using torchvision, the ResNet model uses batch normalization by default.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<div style="background-color:#40C4FF;color: #FFFFFF; padding: 5px; font-weight:bold; font-size:14px;">NOTE</div>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; font-family:monospace; font-size:14px;">
When using torchvision ResNet on image size smaller than 224 x 224, we make the following modifications:
<pre><code class="language-python">model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False)
model.maxpool = torch.nn.Identity()
</code></pre>
</div>
<div style="break-before: page; page-break-before: always;"></div><h2 id="lenet"><a class="header" href="#lenet">LeNet</a></h2>
<p>Our <a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/networks.py">implementation</a> of LeNet is based on <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a>.</p>
<p>We provide the following interface to initialize a LeNet model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_lenet(model_name: str, im_size: tuple, channel: int, num_classes: int, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-10"><a class="header" href="#parameters-10">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="vgg"><a class="header" href="#vgg">VGG</a></h2>
<p>DD-Ranking supports implementation of VGG in both <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a> and <a href="https://pytorch.org/vision/main/models/vgg.html">torchvision</a>.</p>
<p>We provide the following interface to initialize a ConvNet model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_vgg(model_name: str,
im_size: tuple, channel: int, num_classes: int, depth: int, batchnorm: bool, use_torchvision: bool, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-11"><a class="header" href="#parameters-11">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>depth</strong>(<span style="color:#FF6B00;">int</span>): Depth of the network.</li>
<li><strong>batchnorm</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use batch normalization.</li>
<li><strong>use_torchvision</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use torchvision to initialize the model.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<div style="background-color:#40C4FF;color: #FFFFFF; padding: 5px; font-weight:bold; font-size:14px;">NOTE</div>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; font-family:monospace; font-size:14px;">
When using torchvision VGG on image size smaller than 224 x 224, we make the following modifications:
<p>For 32x32 image size:</p>
<pre><code class="language-python">model.classifier = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(512 * 1 * 1, 4096)),
    ('relu1', nn.ReLU(True)),
    ('drop1', nn.Dropout()),
    ('fc2', nn.Linear(4096, 4096)),
    ('relu2', nn.ReLU(True)),
    ('drop2', nn.Dropout()),
    ('fc3', nn.Linear(4096, num_classes)),
]))
</code></pre>
<p>For 64x64 image size:</p>
<pre><code class="language-python">model.classifier = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(512 * 2 * 2, 4096)),
    ('relu1', nn.ReLU(True)),
    ('drop1', nn.Dropout()),
    ('fc2', nn.Linear(4096, 4096)),
    ('relu2', nn.ReLU(True)),
    ('drop2', nn.Dropout()),
    ('fc3', nn.Linear(4096, num_classes)),
]))
</code></pre>
</div><div style="break-before: page; page-break-before: always;"></div><h2 id="mlp"><a class="header" href="#mlp">MLP</a></h2>
<p>Our <a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/networks.py">implementation</a> of MLP is based on <a href="https://github.com/VICO-UoE/DatasetCondensation">DC</a>.</p>
<p>We provide the following interface to initialize a MLP model:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_mlp(model_name: str, im_size: tuple, channel: int, num_classes: int, pretrained: bool, model_path: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/models.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-12"><a class="header" href="#parameters-12">Parameters</a></h3>
<ul>
<li><strong>model_name</strong>(<span style="color:#FF6B00;">str</span>): Name of the model. Please navigate to <a href="models/models/overview.html">models</a> for the model naming convention in DD-Ranking.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>channel</strong>(<span style="color:#FF6B00;">int</span>): Number of channels of the input image.</li>
<li><strong>num_classes</strong>(<span style="color:#FF6B00;">int</span>): Number of classes.</li>
<li><strong>pretrained</strong>(<span style="color:#FF6B00;">bool</span>): Whether to load pretrained weights.</li>
<li><strong>model_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the pretrained model weights.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="datasets"><a class="header" href="#datasets">Datasets</a></h1>
<p>DD-Ranking provides a set of commonly used datasets in existing dataset distillation methods. Users can flexibly use these datasets for evaluation. The interface to load datasets is as follows:</p>
<div style="background-color:#F7F7F7; padding:15px; border:1px solid #E0E0E0; border-top:3px solid #FF0000; font-family:monospace; font-size:14px;">
<p>dd_ranking.utils.get_dataset(dataset: str, data_path: str, im_size: tuple, use_zca: bool, custom_val_trans: Optional[Callable], device: str)
<a href="https://github.com/NUS-HPC-AI-Lab/DD-Ranking/blob/main/dd_ranking/utils/data.py"><strong>[SOURCE]</strong></a></p>
</div>
<h3 id="parameters-13"><a class="header" href="#parameters-13">Parameters</a></h3>
<ul>
<li><strong>dataset</strong>(<span style="color:#FF6B00;">str</span>): Name of the dataset.</li>
<li><strong>data_path</strong>(<span style="color:#FF6B00;">str</span>): Path to the dataset.</li>
<li><strong>im_size</strong>(<span style="color:#FF6B00;">tuple</span>): Image size.</li>
<li><strong>use_zca</strong>(<span style="color:#FF6B00;">bool</span>): Whether to use ZCA whitening. When set to True, the dataset will <strong>not be</strong> normalized using the mean and standard deviation of the training set.</li>
<li><strong>custom_val_trans</strong>(<span style="color:#FF6B00;">Optional[Callable]</span>): Custom transformation on the validation set.</li>
<li><strong>device</strong>(<span style="color:#FF6B00;">str</span>): Device for performing ZCA whitening.</li>
</ul>
<p>Currently, we support the following datasets with default settings. We will keep updating this section with more datasets.</p>
<ul>
<li><strong>CIFAR10</strong>
<ul>
<li><strong>channels</strong>: <code>3</code></li>
<li><strong>im_size</strong>: <code>(32, 32)</code></li>
<li><strong>num_classes</strong>: <code>10</code></li>
<li><strong>mean</strong>: <code>[0.4914, 0.4822, 0.4465]</code></li>
<li><strong>std</strong>: <code>[0.2023, 0.1994, 0.2010]</code></li>
</ul>
</li>
<li><strong>CIFAR100</strong>
<ul>
<li><strong>channels</strong>: <code>3</code></li>
<li><strong>im_size</strong>: <code>(32, 32)</code></li>
<li><strong>num_classes</strong>: <code>100</code></li>
<li><strong>mean</strong>: <code>[0.4914, 0.4822, 0.4465]</code></li>
<li><strong>std</strong>: <code>[0.2023, 0.1994, 0.2010]</code></li>
</ul>
</li>
<li><strong>TinyImageNet</strong>
<ul>
<li><strong>channels</strong>: <code>3</code></li>
<li><strong>im_size</strong>: <code>(64, 64)</code></li>
<li><strong>num_classes</strong>: <code>200</code></li>
<li><strong>mean</strong>: <code>[0.485, 0.456, 0.406]</code></li>
<li><strong>std</strong>: <code>[0.229, 0.224, 0.225]</code></li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
