<center>

# DD-Ranking

</center>

## Motivation

Dataset Distillation (DD) aims to condense a large dataset into a much smaller one, which allows a model to achieve comparable performance after training on it. DD has gained extensive attention since it was proposed. With some foundational methods such as DC, DM, and MTT, various works have further pushed this area to a new standard with their novel designs.

Notebaly, more and more methods are transitting from "hard label" to "soft label" in dataset distillation, especially during evaluation.**Hard labels** are categorical, having the same format of the real dataset. **Soft labels** are distributions, typically generated by a pre-trained teacher model.

However, we notice that DD lacks a unified and fair evaluation benchmark. Issues of current evaluation scheme are summarized as follows:

1. Results of using hard and soft labels are not directly comparable. The essence of using soft labels is **knowledge distillation**. When introducing teacher knowledge to train a model on the distilled dataset, the obtained test accuracy may not fully reflect the pure informativeness of the distilled data.
2. Strategies of using soft labels are diverse. We have seen different objective functions during evaluation, such as soft Cross-Entropy and Kullbackâ€“Leibler divergence. Since the objective function to use soft labels is usually not a contribution of most methods, it is not fair to compare different methods with different soft label strategies.
3. Data augmentations on distilled datasets are diverse. Different methods may adopt different data augmentations to enhance the model training, which could improve the test accuracy to different extents. Thus, data augmentation should also be properly aligned.

With above issues, we point out the major limitation of the current DD evaluation as follows: **the test accuracy of the model trained on distilled data does not equal to the real informativeness of the distilled data.**

## DD-Ranking

